{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTIONS NOTEBOOK\n",
    "\n",
    "In this tutorial we use [requests](https://pypi.org/project/requests/) and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) Python packages to look at information in 3 BBC articles.\n",
    "\n",
    "Disclaimer: Note that you should always double check a website's Terms of Service and double check with the website owner that you're allowed to scrape. This tutorial is just a research exercise - we're not actually scraping any articles data from BBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News article URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bbc.co.uk/news/business-65321487\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the HTML source code with requests\n",
    "\n",
    "We start by using the requests package to get the HTML source code for the URL above.\n",
    "\n",
    "Each time you do `requests.get()` you're accessing the website once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_html = requests.get(url = url)\n",
    "soup = BeautifulSoup(markup = page_html.content, features = \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can (and should) declare who you are in headers through the user agent, for transparency purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_html = requests.get(url = url, headers = {'User-Agent':\"Data collection for the purpose of research. For questions contact: xx@xx.com\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 200 response/status code means that the request was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytes variable\n",
    "type(page_html.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(markup = page_html.content, features = \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of `print()` and the `prettify()` functions to better understand the HTML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using prettify and print help with understanding the HTML since they add indentation\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us now extract information from our \"soup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Heading\n",
    "\n",
    "`find()` will return the first matching item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find(id=\"main-heading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body of text\n",
    "\n",
    "Let us start by using `find()` to get the first matching item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(name='div', attrs={'data-component':\"text-block\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_match = soup.find(name='div', attrs={'data-component':\"text-block\"})\n",
    "print(first_match.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_match.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all()` returns a list of matching items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full list of matching items\n",
    "all_matches = soup.find_all(name='div', attrs={'data-component':\"text-block\"})\n",
    "all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List size\n",
    "len(all_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of text strings\n",
    "[t.text for t in all_matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All sentences together\n",
    "print(\" \".join([t.text for t in all_matches]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time as text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_info = soup.find_all(name='time')\n",
    "time_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_info[0].prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably BST time\n",
    "soup.find(name='time').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time as datime (UTC time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(name='time').attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(name='time').attrs[\"datetime\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All links found on the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First link\n",
    "soup.find(name=\"a\").get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing all links found on the webpage\n",
    "for link in soup.find_all(name='a'):\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of topic links\n",
    "all_links = soup.find_all(name='a')\n",
    "topic_links = [link.get('href') for link in all_links if \"topics\" in link.get('href')]\n",
    "topic_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic names list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics all in one string without a clear separator\n",
    "[i.text for i in soup.find_all(name='div', attrs={'data-component':\"topic-list\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(name='div', attrs={'data-component':\"topic-list\"}).find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[topic.text for topic in soup.find(name='div', attrs={'data-component':\"topic-list\"}).find_all(\"li\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only really want the author\n",
    "soup.find(name='div', attrs={'data-component':\"byline-block\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by = soup.find(name='div', attrs={'data-component':\"byline-block\"})\n",
    "print(by.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by.find(\"div\", attrs={\"class\":\"ssrcss-68pt20-Text-TextContributorName e8mq1e96\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Does the code above work on other articles from bbc.co.uk? \n",
    "\n",
    "You can give it a try with the following ones or others:\n",
    "\n",
    "```\n",
    "url = \"https://www.bbc.co.uk/news/science-environment-57159056\"\n",
    "url = \"https://www.bbc.co.uk/news/business-64261457\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does work on these 3 webpages - that doesn't mean it will work in all remaining pages, but when creating a scraper you should aim for it to be as general as possible if you aim to collect data from multiple similar pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. This article `https://www.bbc.co.uk/news/science-environment-57159056` contains headlines (besides the main title). Write code to extract those headlines.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "a) Use requests to get the HTML source code\n",
    "\n",
    "b) Use BeautifulSoup to parse the HTML.\n",
    "\n",
    "c) Build a rule that allows you to extract the information you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.bbc.co.uk/news/science-environment-57159056\"\n",
    "page_html = requests.get(url = url)\n",
    "soup = BeautifulSoup(markup = page_html.content, features = \"html.parser\")\n",
    "soup.find_all('div', attrs={'data-component':\"subheadline-block\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('div', attrs={'data-component':\"subheadline-block\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract the figure captions from one of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_fig_caption = soup.find('figcaption')\n",
    "print(first_fig_caption.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('figcaption').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fig_captions = soup.find_all('figcaption')\n",
    "\n",
    "[caption.text for caption in all_fig_captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extract relevant metadata information from the pictures in one of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.find('picture').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can for example extract a picture Alt text\n",
    "soup.find('picture').findChildren()[1].attrs[\"alt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How do you interpret the following `robots.txt` files?\n",
    "\n",
    "\n",
    "**Robots #1:**\n",
    "\n",
    "```\n",
    "User-agent: Google\n",
    "Disallow:\n",
    "\n",
    "User-agent: *\n",
    "Disallow: /\n",
    "```\n",
    "\n",
    "**Robots #2:**\n",
    "\n",
    "```\n",
    "User-agent: BadBot\n",
    "Disallow: /\n",
    "\n",
    "User-agent: *\n",
    "Disallow: /search/\n",
    "Request-rate: 15/100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robots 1:\n",
    "- Google is allowed to crawl all website pages with no restrictions;\n",
    "- All other bots are not allowed to access any page of the website;\n",
    "\n",
    "Robots 2:\n",
    "- BadBot can't access any page;\n",
    "- All other user agents can access all paths except `/search/`. In all other paths there should be only 15 requests every 100 seconds (roughly 1 request every 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f35cc60ea0712af2775b3f400f38ffdc8227df7d9b3c2955aec2b2f8cec1696"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
