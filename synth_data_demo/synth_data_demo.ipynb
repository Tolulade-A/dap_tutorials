{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from data_getters import load_s3_data\n",
    "\n",
    "BUCKET_NAME = \"dsp-tutorials\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "For today, we're using a cleaned dataset, to focus on data generation and not data cleaning. However, the cleaning steps are described FYI below, and code used to clean the dataset can be found here (dap_tutorials/synth_data_demo/preprocessing_df.py):\n",
    "\n",
    "1. The synthetic data generators will mimic all statistical relationships they can find in the data. So:\n",
    "    - Drop any numeric 'ids' in the data, unless the ordering of the observations matters! It will really eat into the fidelity of the data if the generator tries to mimic the ordering of the ids\n",
    "    - Make sure you change any 'numeric' categorical variables to strings, to ensure they are treated as non-ordered variables. Whilst you can just define the datatype, I found this was a bit buggy. It was easier to just replace numbers with letters in the data. \n",
    "\n",
    "\n",
    "2. 'Missingness' in this context is an important statistical feature that we want to preserve. However, the many packages  won't accept nan values:\n",
    "    - For categorical variables, we can just replace nan with a new category ('NA' for example)\n",
    "    - For continuous variable, we need to impute values to maintain the original statistical properties of the variable, and then create a new binary column that indicates if the original value was missing (so that the generator learns both the correct statistical relationships, and the 'missingness' pattern)\n",
    "\n",
    "3. Some generators struggle with a large number of categories within a single variable. So take note of how many categories there are - if you have issues with training a model later on, this will be a good place to start optimising. \n",
    "\n",
    "4. Maybe goes without saying, but:\n",
    "    - You can't generate synthetic free text fields (eg, comments from a survey) with tabular data generation methods, like we're using today.  So drop any before you start.\n",
    "    - All var types need to reflect the data type (numbers are ints or floats, categorical variables are bools or objects)\n",
    "    \n",
    "5. There is a tradeoff of how well statistical relationships are preserved (think of it like allocating a 'fidelity buget' across different variables). If there are columns that are really irrelevant to your analysis or final dataset, you could drop them before you start (although be very careful doing this, in case it's a confounding variable).Alternatively, there are methods that allow you to nominate the priority fields (although we aren't using them today). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the cleaned data and explore\n",
    "\n",
    "#Load pre-cleaned dataset from S3 (we will create a synthetic version from this)\n",
    "\n",
    "orig_df = load_s3_data(BUCKET_NAME,'synthetic_data_generation/magic_breakfast_cleaned_data.csv')\n",
    "\n",
    "\n",
    "print('DataFrame columns:', orig_df.columns)\n",
    "print('DataFrame shape:', orig_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(orig_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data model - Maximum Spanning Tree\n",
    "\n",
    "* MST is a graph-based method for generating tabular synthetic data. It finds the marginal distribution for variables in the sample (the probability that a value occurs for a single variable, independent of the other variables). \n",
    "* Then, it adds some noise to the marginal distribution (this is the step that introduces DP; and the amount of noise is controlled by the privacy threshold ). \n",
    "* Finally, it generates synthetic data that preserves the marginal distributions.\n",
    "* Pros: \n",
    "    - High fidelity method\n",
    "    - Auditable (not a black box model)\n",
    "    - Can allocate the privacy budget to specific variables, enabling greater preservation of some relationships, and more noise in others\n",
    "* Cons: \n",
    "    - Requires discrete values (like all graph-based methods), so any continuous variables would need to be converted to discrete categories before generating synthetic data. However, the method is able to take such a high number of discrete values that data can be ‘binned’ (grouped together) into very small ranges (it was still an effective method on the UK census, with a number of continuous variables including ages and dates, with 55 million observations each). \n",
    "    - Slightly more of a manual process to set up than some other methods, as you need to define the privacy budget for each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "from mbi import Dataset, FactoredInference, Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the 'domain' file - an input of each column and count of their unique values required to train the MST generator\n",
    "\n",
    "def create_domain(df: pd.DataFrame, save: bool = True) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a domain dictionary of the format {column name: count of unique values} for each column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        save (bool): Flag indicating whether to save the domain dictionary as a JSON file. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        dict: The domain dictionary with column names as keys and the number of unique values as values.\n",
    "    \"\"\"\n",
    "\n",
    "    domain_dict = {col: df[col].nunique() for col in df.columns}\n",
    "\n",
    "    if save:\n",
    "        with open(\"domain.json\", \"w\") as f:\n",
    "            json.dump(domain_dict, f)\n",
    "\n",
    "    return domain_dict\n",
    "\n",
    "\n",
    "def load_data_mst(df: pd.DataFrame, domain_file: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    The package only takes locally saved data as inputs - so this function first dumps the data to a CSV file and then loads it.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data_file (str): The path to the raw data file.\n",
    "    domain_file (str): The path to the domain file.\n",
    "\n",
    "    Returns:\n",
    "    data (Dataset): The loaded dataset.\n",
    "    domain (Domain): The domain of the loaded dataset.\n",
    "    \"\"\"\n",
    "    raw_data_file = \"magic_breakfast.csv\"\n",
    "    df.to_csv(raw_data_file)\n",
    "    data = Dataset.load(raw_data_file, domain_file)\n",
    "    domain = data.domain\n",
    "    return data, domain\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create domain and load data\n",
    "create_domain(orig_df)\n",
    "data, domain = load_data_mst(orig_df, 'domain.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Allocate privacy budget and train model\n",
    "'''\n",
    "Define the cliques - the relationships between variables that will have less noise allocated to them. \n",
    "For an RCT, the relationships we most want to preserve are between the allocation to the control and treatment groups and the post-test outcomes.'''\n",
    "\n",
    "CLIQUES = [\n",
    "    (\"Treatment_Allocation\", \"PostTest_Outcome_1\"),\n",
    "    (\"Treatment_Allocation\", \"PostTest_Outcome_2\"),\n",
    "]\n",
    "\n",
    "def allocate_privacy_budget(data: Dataset, cliques: list, total_epsilon:int=1) -> list:\n",
    "    \"\"\"\n",
    "    Allocates privacy budget for measuring marginals in a dataset.\n",
    "\n",
    "    Args:\n",
    "        data (Dataset): The dataset containing the data.\n",
    "        domain (Domain): The domain of the data.\n",
    "        cliques (list): A list of cliques representing 2 and 3 way marginals\n",
    "        (these are the relationships between variables that will have less noise allocated to them).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of measurements, each containing the measurement matrix, noisy data, noise scale, and marginal information.\n",
    "    \"\"\"\n",
    "\n",
    "    # spend half of privacy budget to measure all 1 way marginals\n",
    "    np.random.seed(0)\n",
    "\n",
    "    epsilon = total_epsilon\n",
    "    epsilon_split = epsilon / (len(data.domain) + len(cliques))\n",
    "    sigma = 2.0 / epsilon_split\n",
    "\n",
    "    measurements = []\n",
    "    for col in data.domain:\n",
    "        x = data.project(col).datavector()\n",
    "        y = x + np.random.laplace(loc=0, scale=sigma, size=x.size)\n",
    "        I = sparse.eye(x.size)\n",
    "        measurements.append((I, y, sigma, (col,)))\n",
    "\n",
    "    # spend half of privacy budget to measure some more 2 and 3 way marginals\n",
    "\n",
    "    for cl in cliques:\n",
    "        x = data.project(cl).datavector()\n",
    "        y = x + np.random.laplace(loc=0, scale=sigma, size=x.size)\n",
    "        I = sparse.eye(x.size)\n",
    "        measurements.append((I, y, sigma, cl))\n",
    "\n",
    "    return measurements\n",
    "\n",
    "\n",
    "def train_model(data: Dataset, domain: Domain, measurements: list, no_iterations:int = 2500):\n",
    "    \"\"\"\n",
    "    Trains a model using the given dataset, domain, and measurements.\n",
    "\n",
    "    Args:\n",
    "        data (Dataset): The dataset to train the model on.\n",
    "        domain (Domain): The domain of the data.\n",
    "        measurements (list): The list of measurements to use for training.\n",
    "\n",
    "    Returns:\n",
    "        model: The trained model.\n",
    "    \"\"\"\n",
    "    engine = FactoredInference(domain, log=True, iters=no_iterations)\n",
    "    total = data.df.shape[0]\n",
    "    model = engine.estimate(measurements, total=total)\n",
    "    return model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set paramaeters and train model (takes ~30s to run)\n",
    "measurements = allocate_privacy_budget(data, CLIQUES)\n",
    "model = train_model(data, domain, measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model, no_rows: int, save: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic data using the given model.\n",
    "\n",
    "    Parameters:\n",
    "    model (Model): The model used to generate synthetic data.\n",
    "    no_rows (int): The number of rows to generate.\n",
    "    save (bool, optional): Whether to save the generated data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The generated synthetic data as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate synthetic data\n",
    "    synthetic_data = model.synthetic_data(rows=no_rows)\n",
    "    synth_df = synthetic_data.df\n",
    "    return synth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate synthetic data\n",
    "\n",
    "mst_synth_data = generate_data(model, len(orig_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the synthetic data\n",
    "\n",
    "There are endless options for evaluating synthetic data. We'll use a package called 'synthgauge' to inspect the data. It was developed by the ONS, and contains well-regarded benhchmarks for tabular synthetic data utility, privacy and fidelity. We chose it because it's from a reputable source in the UK (important to create trust for EEF if they produce high fidelity data), can be applied to data from many sources/ methods, and is easy to implement. \n",
    "\n",
    " More information can be found here:  https://datasciencecampus.github.io/synthgauge/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import synthgauge as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mst_evaluator = sg.Evaluator(orig_df, mst_synth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for data exploration\n",
    "\n",
    "def histograms(evaluator):\n",
    "    return evaluator.plot_histograms(figsize=(15, 30))\n",
    "\n",
    "def cont_data_description (evaluator):\n",
    "    return evaluator.describe_numeric()\n",
    "\n",
    "def cat_data_description (evaluator):\n",
    "    return evaluator.describe_categorical()\n",
    "\n",
    "def correlation_tables(evaluator):\n",
    "    return evaluator.plot_correlation(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst_evaluator.plot_histograms(figsize=(15, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data using these functions, and any other analysis you can think of.\n",
    "- Where does the generator perform well? \n",
    "- What happens if you change the relationships that have a privacy budget allocated to them? (CLIQUES in the allocate_privacy_budget function above)\n",
    "- What happens if you change the total privacy budget? (epsilon in the allocate_privacy_budget above)\n",
    "- What happens if you change the number of iterations? (n_iter in the train_model function above)\n",
    "- Finally, what if you produce different numbers of observations? (no_rows in the generate_data function above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark the MST synthetic data\n",
    "\n",
    "Along with informal inspection of the data, there are formal metrics available to benchmark different models in terms of their privacy, utility, and fidelity. In the project, we examined a number of metrics. For the sake of time, just one is loaded below (although you can add additional metrics if you have time - see documentation here https://datasciencecampus.github.io/synthgauge/autoapi/synthgauge/metrics/index.html)\n",
    "\n",
    "**Fidelity**\n",
    "- **PMSE**: Mean-squared difference in pairwise Pearson correlation coefficients (continuous data) - Measures how well relationships between continuous variables have been preserved, by taking the difference in correlations between variables between the synthetic and original data. *(Minimise towards 0)*\n",
    "- **PMSE-ratio**: Correlation ratio mean-squared difference (continuous/categorical data) - Measures how well relationships between continuous or categorical variables have been preserved, by taking the difference in correlations between variables between the synthetic and original data. *(Minimise towards 0)*\n",
    "- **MAD**: Mean absolute difference of feature densities - Measures how well the distribution of individual variables has been preserved, by taking the difference in the measure of their distribution between synthetic and original data. *(Minimise towards 0)*\n",
    "\n",
    "**Utility**\n",
    "- **Classification error**: Classification comparison (difference in precision, recall, F1 scores on a classification task) - Uses the synthetic data to train a classifier, and then tests the difference in accuracy when predicting on real data. *(Minimize towards 0)*\n",
    "\n",
    "**Privacy** \n",
    "- **Nearest neighbors**: Minimum distance nearest neighbor - A check to ensure that outliers have not been replicated in the synthetic data *(Maximise)*\n",
    "- **TCAP score**: Target Correct Attributional Probability Score - The risk that a target variable can be generated given a key variable *(Minimise towards 0)*\n",
    "- **Sample overlap**: Proportion of real data found in synthetic data - A straightforward check that no real observations are contained in the synthetic dataset (necessary, but not sufficient, to preserve privacy) *(Should always be 0)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcap_test(\n",
    "    original_df: pd.DataFrame, synthetic_df: pd.DataFrame, key: list, target: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function calculates the tcap_score metric - which is the chance that an attacker could infer the\n",
    "    true values of the target variable, if they had acess to both the true and synthetic values of the 'key' variables.\n",
    "    The key variables should therefore be data that is widely available; the target should be sensitive data.\n",
    "\n",
    "    Parameters:\n",
    "        original_df (pd.DataFrame): The original dataframe containing the actual data.\n",
    "        synthetic_df (pd.DataFrame): The synthetic dataframe containing the generated data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The evaluation results as a dataframe.\n",
    "    \"\"\"\n",
    "    evaluator = sg.Evaluator(original_df, synthetic_df)\n",
    "    evaluator.add_metric(\n",
    "        \"tcap_score\",\n",
    "        key=key,\n",
    "        target=target,\n",
    "    )\n",
    "    return evaluator.evaluate(as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcap_test(orig_df, mst_synth_data, [\"Treatment_Allocation\"], \"PostTest_Outcome_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asf_eef_synth_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
